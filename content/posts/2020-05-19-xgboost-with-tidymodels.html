---
title: "Using XGBoost with Tidymodels"
author: "Andy Merlino and Nick Merlino"
date: "2020-05-19"
categories: ["R", "XGBoost", "tidymodels", "Machine Learning"]
tags: ["R", "XGBoost", "tidymodels", "Machine Learning"]
draft: false
image: "https://res.cloudinary.com/dxqnb8xjb/image/upload/v1589830382/xg_tidy_puzzle_pn6p01.png"
intro: 'In this post we will train and tune an XGBoost model using the tidymodels R packages.  We use the AmesHousing dataset which contains housing data from Ames, Iowa. Our model will predict house sale price.'
output:
  html_document
---



<div id="background" class="section level1">
<h1>Background</h1>
<p>XGBoost is a machine learning library originally written in C++ and ported to R in the <span class="inline_code">xgboost</span> R package. Over the last several years, XGBoost’s effectiveness in Kaggle competitions catapulted it in popularity. At Tychobra, XGBoost is our go-to machine learning library.</p>
<p>François Chollet and JJ Allaire summarize the value of XGBoost in the intro to “Deep Learning in R”:</p>
<blockquote>
<p>In 2016 and 2017, Kaggle was dominated by two approaches: gradient boosting machines and deep learning. Specifically, gradient boosting is used for problems where structured data is available, whereas deep learning is used for perceptual problems such as image classification. Practitioners of the former almost always use the excellent XGBoost library.</p>
</blockquote>
<blockquote>
<p>These are the two techniques you should be the most familiar with in order to be successful in applied machine learning today: gradient boosting machines, for shallow-learning problems; and deep learning, for perceptual problems. In technical terms, this means you’ll need to be familiar with XGBoost and Keras—the two libraries that currently dominate Kaggle competitions.</p>
</blockquote>
<p>At Tychobra, we have trained XGBoost models using the <span class="inline_code">caret</span> R package created by Max Kuhn. <span class="inline_code">caret</span> has treated us very well over the years (check out our post <a href="https://www.tychobra.com/posts/claims-ml/">Machine Learning for Insurance Claims</a> for an example of using <span class="inline_code">xgboost</span> with <span class="inline_code">caret</span>).</p>
<p>Max Kuhn and others at Rstudio have more recently turned their attention from <span class="inline_code">caret</span> to “tidymodels” (the successor to <span class="inline_code">caret</span>). “tidymodels” is a collection of R packages that work together to simplify and supercharge model training and tuning. With the recent launch of <a href="https://www.tidymodels.org/">tidymodels.org</a>, we felt it was time to give the tidymodels R packages a shot.</p>
</div>
<div id="overview" class="section level1">
<h1>Overview</h1>
<p>In this post we will train and tune an XGBoost model using the tidymodels R packages. We use the <a href="https://cran.r-project.org/web/packages/AmesHousing/AmesHousing.pdf">AmesHousing</a> dataset which contains housing data from Ames, Iowa. Our model will predict house sale price.</p>
<pre class="r"><code># data
library(AmesHousing)

# data cleaning
library(janitor)

# data prep
library(dplyr)

# tidymodels
library(rsample)
library(recipes)
library(parsnip)
library(tune)
library(dials)
library(workflows)
library(yardstick)

# speed up computation with parrallel processing (optional)
library(doParallel)
all_cores &lt;- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)</code></pre>
<p>Load in the Ames housing dataset.</p>
<pre class="r"><code># set the random seed so we can reproduce any simulated results.
set.seed(1234)

# load the housing data and clean names
ames_data &lt;- make_ames() %&gt;%
  janitor::clean_names()</code></pre>
<div id="step-0-eda-exploratory-data-analaysis" class="section level4">
<h4>Step 0: EDA (Exploratory Data Analaysis)</h4>
<p>At this point we would normally make a few simple plots and summaries of the data to get a high-level understanding of the data. For simplicity, we are going to cut the EDA process from this post, but, in a real-world analysis, understanding the business problems and doing effective EDA are often the most time consuming and crucial aspects of the analysis.</p>
</div>
<div id="step-1-initial-data-split" class="section level4">
<h4>Step 1: Initial Data Split</h4>
<p>Now we split the data into training and test data. Training data is used for the model training and hyperparameter tuning. Once trained, the model can be evaluated against test data to assess accuracy.</p>
<pre class="r"><code># split into training and testing datasets. Stratify by Sale price 
ames_split &lt;- rsample::initial_split(
  ames_data, 
  prop = 0.2, 
  strata = sale_price
)</code></pre>
</div>
<div id="step-2-preprocessing" class="section level4">
<h4>Step 2: Preprocessing</h4>
<p>Preprocessing alters the data to make our model more predictive and the training process less compute intensive. Many models require careful and extensive variable preprocessing to produce accurate predictions. XGBoost, however, is robust against highly skewed and/or correlated data, so the amount of preprocessing required with XGBoost is minimal. Nevertheless, we can still benefit from some preprocessing.</p>
<p>In tidymodels, we use the <span class="inline_code">recipes</span> package to define these preprocessing steps, in what is called a “recipe”.</p>
<pre class="r"><code># preprocessing &quot;recipe&quot;
preprocessing_recipe &lt;- 
  recipes::recipe(sale_price ~ ., data = training(ames_split)) %&gt;%
  # convert categorical variables to factors
  recipes::step_string2factor(all_nominal()) %&gt;%
  # combine low frequency factor levels
  recipes::step_other(all_nominal(), threshold = 0.01) %&gt;%
  # remove no variance predictors which provide no predictive information 
  recipes::step_nzv(all_nominal()) %&gt;%
  prep()</code></pre>
<p>As you can see in the chart below, for the “neighborhood” variable, several of the factor levels with the fewest observations (less than 1% of the total number of observations) have been lumped into an “other” factor level. We did this preprocessing in <span class="inline_code">step_other()</span> in the above recipe.</p>
<p><img src="/posts/2020-05-19-xgboost-with-tidymodels_files/figure-html/bar_chart-1.png" width="672" /></p>
</div>
<div id="step-3-splitting-for-cross-validation" class="section level4">
<h4>Step 3: Splitting for Cross Validation</h4>
<p>We apply our previously defined preprocessing recipe with <span class="inline_code">bake()</span>. Then we use cross-validation to randomly split the training data into further training and test sets. We will use these additional cross validation folds to tune our hyperparameters in a later step.</p>
<pre class="r"><code>ames_cv_folds &lt;- 
  recipes::bake(
    preprocessing_recipe, 
    new_data = training(ames_split)
  ) %&gt;%  
  rsample::vfold_cv(v = 5)</code></pre>
</div>
<div id="step-4-xgboost-model-specification" class="section level4">
<h4>Step 4: XGBoost Model Specification</h4>
<p>We use the <span class="inline_code">parsnip</span> package to define the XGBoost model specification. Below we use <span class="inline_code">boost_tree()</span> along with <span class="inline_code">tune()</span> to define the hyperparameters to undergo tuning in a subsequent step.</p>
<pre class="r"><code># XGBoost model specification
xgboost_model &lt;- 
  parsnip::boost_tree(
    mode = &quot;regression&quot;,
    trees = 1000,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
  ) %&gt;%
    set_engine(&quot;xgboost&quot;, objective = &quot;reg:squarederror&quot;)</code></pre>
</div>
<div id="step-5-grid-specification" class="section level4">
<h4>Step 5: Grid Specification</h4>
<p>We use the tidymodel <span class="inline_code">dials</span> package to specify the parameter set.</p>
<pre class="r"><code># grid specification
xgboost_params &lt;- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )</code></pre>
<p>Next we set up the grid space. The <span class="inline_code">dails::grid_*</span> functions support several methods for defining the grid space. We are using the <span class="inline_code">dails::grid_max_entropy()</span> function which covers the hyperparameter space such that any portion of the space has an observed combination that is not too far from it.</p>
<pre class="r"><code>xgboost_grid &lt;- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 60
  )

knitr::kable(head(xgboost_grid))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">min_n</th>
<th align="right">tree_depth</th>
<th align="right">learn_rate</th>
<th align="right">loss_reduction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">11</td>
<td align="right">6</td>
<td align="right">0.0091690</td>
<td align="right">0.0000001</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="right">7</td>
<td align="right">0.0346875</td>
<td align="right">0.0451186</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">6</td>
<td align="right">0.0000018</td>
<td align="right">0.3011571</td>
</tr>
<tr class="even">
<td align="right">37</td>
<td align="right">10</td>
<td align="right">0.0000001</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="right">29</td>
<td align="right">1</td>
<td align="right">0.0022864</td>
<td align="right">0.2903964</td>
</tr>
<tr class="even">
<td align="right">38</td>
<td align="right">6</td>
<td align="right">0.0003792</td>
<td align="right">0.0000000</td>
</tr>
</tbody>
</table>
<p>
</p>
<p>To tune our model, we perform grid search over our <span class="inline_code">xgboost_grid</span>’s grid space to identify the hyperparameter values that have the lowest prediction error.</p>
</div>
<div id="step-6-define-the-workflow" class="section level4">
<h4>Step 6: Define the Workflow</h4>
<p>We use the new tidymodel <span class="inline_code">workflows</span> package to add a formula to our XGBoost model specification.</p>
<pre class="r"><code>xgboost_wf &lt;- 
  workflows::workflow() %&gt;%
  add_model(xgboost_model) %&gt;% 
  add_formula(sale_price ~ .)</code></pre>
</div>
<div id="step-7-tune-the-model" class="section level4">
<h4>Step 7: Tune the Model</h4>
<p>Tuning is where the <span class="inline_code">tidymodels</span> ecosystem of packages really comes together. Here is a quick breakdown of the objects passed to the first 4 arguments of our call to <span class="inline_code">tune_grid()</span> below:</p>
<ul>
<li>“object”: <span class="inline_code">xgboost_wf</span> which is a workflow that we defined by the <span class="inline_code">parsnip</span> and <span class="inline_code">workflows</span> packages</li>
<li>“resamples”: <span class="inline_code">ames_cv_folds</span> as defined by <span class="inline_code">rsample</span> and <span class="inline_code">recipes</span> packages</li>
<li>“grid”: <span class="inline_code">xgboost_grid</span> our grid space as defined by the <span class="inline_code">dials</span> package</li>
<li>“metric”: the <span class="inline_code">yardstick</span> package defines the metric set used to evaluate model performance</li>
</ul>
<pre class="r"><code># hyperparameter tuning
xgboost_tuned &lt;- tune::tune_grid(
  object = xgboost_wf,
  resamples = ames_cv_folds,
  grid = xgboost_grid,
  metrics = yardstick::metric_set(rmse, rsq, mae),
  control = tune::control_grid(verbose = TRUE)
)</code></pre>
<p>In the above code block <span class="inline_code">tune_grid()</span> performed grid search over all our (60) grid parameter combinations defined in <span class="inline_code">xgboost_grid</span> and used 5 fold cross validation along with rmse (Root Mean Squared Error), rsq (R Squared), and mae (Mean Absolute Error) to measure prediction accuracy. So our tidymodels tuning just fit 60 X 5 = 300 XGBoost models each with 1,000 trees all in search of the optimal hyperparameters. Don’t try that on your TI-83!</p>
<p>These are the hyperparameter values which performed best at minimizing RMSE.</p>
<pre class="r"><code>xgboost_tuned %&gt;%
  tune::show_best(metric = &quot;rmse&quot;) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">min_n</th>
<th align="right">tree_depth</th>
<th align="right">learn_rate</th>
<th align="right">loss_reduction</th>
<th align="left">.metric</th>
<th align="left">.estimator</th>
<th align="right">mean</th>
<th align="right">n</th>
<th align="right">std_err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">12</td>
<td align="right">7</td>
<td align="right">0.0346875</td>
<td align="right">0.0451186</td>
<td align="left">rmse</td>
<td align="left">standard</td>
<td align="right">25561.99</td>
<td align="right">5</td>
<td align="right">2983.927</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">13</td>
<td align="right">0.0183617</td>
<td align="right">0.1042750</td>
<td align="left">rmse</td>
<td align="left">standard</td>
<td align="right">25576.99</td>
<td align="right">5</td>
<td align="right">2687.849</td>
</tr>
<tr class="odd">
<td align="right">23</td>
<td align="right">5</td>
<td align="right">0.0788798</td>
<td align="right">0.7513677</td>
<td align="left">rmse</td>
<td align="left">standard</td>
<td align="right">25645.38</td>
<td align="right">5</td>
<td align="right">2461.057</td>
</tr>
<tr class="even">
<td align="right">11</td>
<td align="right">6</td>
<td align="right">0.0091690</td>
<td align="right">0.0000001</td>
<td align="left">rmse</td>
<td align="left">standard</td>
<td align="right">25669.84</td>
<td align="right">5</td>
<td align="right">2706.457</td>
</tr>
<tr class="odd">
<td align="right">10</td>
<td align="right">2</td>
<td align="right">0.0108475</td>
<td align="right">0.0000003</td>
<td align="left">rmse</td>
<td align="left">standard</td>
<td align="right">25883.23</td>
<td align="right">5</td>
<td align="right">2828.172</td>
</tr>
</tbody>
</table>
<p>
</p>
<p>Next, isolate the best performing hyperparameter values.</p>
<pre class="r"><code>xgboost_best_params &lt;- xgboost_tuned %&gt;%
  tune::select_best(&quot;rmse&quot;)

knitr::kable(xgboost_best_params)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">min_n</th>
<th align="right">tree_depth</th>
<th align="right">learn_rate</th>
<th align="right">loss_reduction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">12</td>
<td align="right">7</td>
<td align="right">0.0346875</td>
<td align="right">0.0451186</td>
</tr>
</tbody>
</table>
<p>
</p>
<p>Finalize the XGBoost model to use the best tuning parameters.</p>
<pre class="r"><code>xgboost_model_final &lt;- xgboost_model %&gt;% 
  finalize_model(xgboost_best_params)</code></pre>
</div>
<div id="step-8-evaluate-performance-of-parameters-on-test-data" class="section level4">
<h4>Step 8: Evaluate Performance of Parameters on Test Data</h4>
<p>Now that we have trained our model, we need to evaluate the model performance. We use test data from step 1 (data that was not used in the model training) to evaluate performance.</p>
<p>We use the <span class="inline_code">rmse</span> (Root Mean Squared Error), <span class="inline_code">rsq</span> (R Squared), and <span class="inline_code">mae</span> (Mean Absolute Value) metrics from the <span class="inline_code">yardstick</span> package in our model evaluation.</p>
<p>First let’s evaluate the metrics on the training data:</p>
<pre class="r"><code>train_processed &lt;- bake(preprocessing_recipe,  new_data = training(ames_split))

train_prediction &lt;- xgboost_model_final %&gt;%
  # fit the model on all the training data
  fit(
    formula = sale_price ~ ., 
    data    = train_processed
  ) %&gt;%
  # predict the sale prices for the training data
  predict(new_data = train_processed) %&gt;%
  bind_cols(training(ames_split))

xgboost_score_train &lt;- 
  train_prediction %&gt;%
  yardstick::metrics(sale_price, .pred) %&gt;%
  mutate(.estimate = format(round(.estimate, 2), big.mark = &quot;,&quot;))

knitr::kable(xgboost_score_train)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">.metric</th>
<th align="left">.estimator</th>
<th align="left">.estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">rmse</td>
<td align="left">standard</td>
<td align="left">2,980.56</td>
</tr>
<tr class="even">
<td align="left">rsq</td>
<td align="left">standard</td>
<td align="left">1.00</td>
</tr>
<tr class="odd">
<td align="left">mae</td>
<td align="left">standard</td>
<td align="left">1,802.76</td>
</tr>
</tbody>
</table>
<p>
</p>
<p>And now for the test data:</p>
<pre class="r"><code>test_processed  &lt;- bake(preprocessing_recipe, new_data = testing(ames_split))

test_prediction &lt;- xgboost_model_final %&gt;%
  # fit the model on all the training data
  fit(
    formula = sale_price ~ ., 
    data    = train_processed
  ) %&gt;%
  # use the training model fit to predict the test data
  predict(new_data = test_processed) %&gt;%
  bind_cols(testing(ames_split))

# measure the accuracy of our model using `yardstick`
xgboost_score &lt;- 
  test_prediction %&gt;%
  yardstick::metrics(sale_price, .pred) %&gt;%
  mutate(.estimate = format(round(.estimate, 2), big.mark = &quot;,&quot;))

knitr::kable(xgboost_score)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">.metric</th>
<th align="left">.estimator</th>
<th align="left">.estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">rmse</td>
<td align="left">standard</td>
<td align="left">31,606.93</td>
</tr>
<tr class="even">
<td align="left">rsq</td>
<td align="left">standard</td>
<td align="left">0.85</td>
</tr>
<tr class="odd">
<td align="left">mae</td>
<td align="left">standard</td>
<td align="left">18,654.97</td>
</tr>
</tbody>
</table>
<p>
</p>
<p>The above metrics on the test data are significantly worse than our training data metrics, so we know that there is some overfitting going on in our model. This highlights the importance of using test data, rather than training data, to evaluate model performance.</p>
<p>To quickly check that there is not an obvious issue with our model’s predictions, let’s plot the test data residuals.</p>
<pre class="r"><code>house_prediction_residual &lt;- test_prediction %&gt;%
  arrange(.pred) %&gt;%
  mutate(residual_pct = (sale_price - .pred) / .pred) %&gt;%
  select(.pred, residual_pct)

ggplot(house_prediction_residual, aes(x = .pred, y = residual_pct)) +
  geom_point() +
  xlab(&quot;Predicted Sale Price&quot;) +
  ylab(&quot;Residual (%)&quot;) +
  scale_x_continuous(labels = scales::dollar_format()) +
  scale_y_continuous(labels = scales::percent)</code></pre>
<p><img src="/posts/2020-05-19-xgboost-with-tidymodels_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The above chart does not show any super obvious trends in the residuals. This indicates that, at a very high level, our model is not systematically making inaccurate predictions for houses with certain predicted sale prices. We would do more model validation here for a real-world analysis, but, for the sake of this post, the above chart is good enough for us.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>In this post, we were not overly concerned with our model’s performance. Our goal was to simply work through the process of training an XGBoost model using tidymodels, and to learn the tidymodels basics along the way.</p>
<p>Tidymodels gives us a standard process and vocabulary to handle resampling (<span class="inline_code">rsample</span>), data preprocessing (<span class="inline_code">recipes</span>), model specification (<span class="inline_code">parsnip</span>), tuning (<span class="inline_code">tune</span>), and model validation (<span class="inline_code">yardstick</span>). The work done by the tidymodels team to “tidy” the machine learning process is a step change improvement for approachability to machine learning in R; it is easier than ever to train and (more importantly) understand the model training process using the tidymodels packages. Thank you tidymodels team!</p>
<p>We are still just getting started with tidymodels, so please let me know if you see errors or have suggestions for improvements!</p>
</div>
<div id="similar-work" class="section level1">
<h1>Similar Work</h1>
<p>If you liked this post, you should check out <a href="https://www.business-science.io/code-tools/2020/01/21/hyperparamater-tune-product-price-prediction.html">this excellent post by Business Science</a> which goes into a lot more detail in the EDA process and also trains a glmnet model.</p>
</div>
