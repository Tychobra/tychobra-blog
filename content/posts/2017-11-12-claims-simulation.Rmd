---
title: "Simulating Insurance Claims"
author: "Andy Merlino"
date: "2017-11-12"
categories: ["R"]
tags: ["R", "tuorial", "simulation", "actuarial"]
draft: false
image: "https://res.cloudinary.com/dxqnb8xjb/image/upload/c_fill,h_400,w_400/v1510529538/matrix-434035_640_pvewnf.jpg"
intro: "When I was fresh out of college, my boss asked me to use R to run simulations.  This was my first introduction to R, and I was a little reluctant to get started.  But with some Googling, I overcame the initial frustrations and am now about the biggest R fanboy around.  It turns out that random simulation serves as a nice introdution to R.  This tutorial gives a basic introduction to simulations in R."
---

```{r setup, include=FALSE}
options(scipen=999)
knitr::opts_chunk$set(collapse = TRUE)
```

When I was fresh out of college, my boss asked me to run some simulations in R.  This was my first introduction to R, and I was a little reluctant to get started.  But with some Googling, I overcame the initial frustrations and am now about the biggest R fanboy around.  It turns out that random simulation serves as a nice introdution to R because it is relatively simple to produce useful output.

In this tutorial we will run simulations very similar to the simulations I ran when first learning R (except the code will be a lot nicer as I was a complete noob at that time).  Our simulations will model insurance claims.

### Step 1: Structure the Simulation

Before we execute any R code, we need to describe the problem a little more fully.  Let's imagine that we own an insurance company and we are writing `100` identical insurance policies next year.  We want to measure the losses we expect to pay on claims resulting from these policies. Our actuary told us the probability of a single policy having a claim is `10%`. Therefore we expect an average of `10` claims `10 = 100 * 0.1`.  We can use a discrete probability distribution to simulate the frequency (number of claims) based on this `10` claim average.  

We then need to simulate a loss amount for each claim.  We can use a continuous probability distribution for this simulation.  Luckily our actuary already told us claim severities follow a lognormal distribution with a log mean of 9 and a log standard deviation of 1.75 (In a real world simulation we could measure how well various distributions fit historical claims experience using several options available in R, but that is beyond the scope of this tutorial). 

### Step 2: Run a Single Frequency/Severity Observation

We will refer to the combination of a frequency simulation and the accompanying severity simulations as an observation.  here we run one observation:

```{r simple_sim}
# set the seed so we can reproduce the simulation
set.seed(1234)

expected_freq <- 10

# generate a single frequency from the poisson distribution
freq <- rpois(n = 1, lambda = expected_freq)

# generate `freq` severities.  Each severity represents the ultimate value of 1 claim 
# we will use the lognormal distribution here
sev <- rlnorm(n = freq, meanlog = 9, sdlog = 1.75)

# view the results
sev
```

In the above example our frequency was `r freq`, and our severities for each claim were `r format(round(sev, 0), big.mark = ",")`.

### Step 3: Run Many Observations

Now that we know how to run a single observation we need to randomly generate a bunch of observations.  By inspecting the results of many observations we can determine the confidence level of experiencing different loss amounts. 

```{r many_sims, message=FALSE}
library(dplyr)
library(purrr)

# number of observations
n <- 1000
# generate many frequencies from the poisson distribution
freqs <- rpois(n = n, lambda = 5)

# generate `freq` severities.  Each severity represents the ultimate value of 1 claim 
obs <- purrr::map(freqs, function(freq) rlnorm(n = freq, meanlog = 9, sdlog = 1.75))

# tidy up the data
i <- 0
obs <- purrr::map(obs, function(sev) {
  i <<- i + 1
  data_frame(
    ob = i,
    sev = sev
  )
})

obs <- dplyr::bind_rows(obs)
head(obs, 10)
```

### Step 4: Initial Inspection of Results

##### 4.1: Plot the distribution of oberservations

```{r, message = FALSE}
library(ggplot2)

# aggregate claim severities by observation
obs_total <- obs %>%
  group_by(ob) %>%
  summarise(sev = sum(sev))

ggplot(obs_total, aes(sev)) +
  geom_histogram()
```

##### 4.2: View some quantiles and summary info

```{r}
quantile(obs_total$sev, probs = c(0.5, 0.75, 0.9, 0.95, 0.99))

summary(obs_total$sev)
```

### Step 5: Alter the simulation to specific needs

Assuming our initial inspection of the output in step 4 went as planned, we are ready to customize our simulation. We could adjust our simulation in a variety of different ways.  Some common paths to take from here are: 

##### 5.1 Capping claims at individual claim limits

```{r}
# cap claims at 250K
obs <- obs %>%
  dplyr::mutate(sev_250K = if_else(sev > 250000, 250000, sev))
```

##### 5.2 Capping observations at an aggregate stop loss

```{r}
# cap each observation at an aggregate stop loss
stop_loss <- 1000000

obs_agg <- obs %>%
  dplyr::group_by(ob) %>%
  dplyr::summarise(
    sev = sum(sev),
    sev_250K = sum(sev_250K)) %>%
  dplyr::mutate(
    sev_agg = if_else(sev > stop_loss, stop_loss, sev),
    sev_capped_agg = if_else(sev_250K > stop_loss, stop_loss, sev_250K)
  )

summary(obs_agg)
```

##### 5.3: Simulate multiple types of claims

There is no reason the claims have to all be generated from the same distributions. We could simulate claims based on many different possible frequency and severity distributions. We could also add parameter risk to our frequency and severity selections.  We could add deductibles, quota share reinsurance, and a variety of other alterations based on our specific needs.

### Conclusion

Simulations are a fun way to get started in R, and with a little imagination simulations allow you to answer highly complicated problems that would be impossible to solve directly from math equations and probability density functions.
